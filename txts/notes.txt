Literature and documentation say that Lasso LARS iteratively adds the feature(s) with highest correlation, but that is not the same as selecting n best-correlating features without iterative refining.
My initial idea for that was to repeatedly draw subsets of a certain size from our data, train a Lasso LARS regression on that and later compare the selected features and spearman correlation ranks


how do we use git
reusable, modular code? reproducible analysis / "results" 
why were the different approaches chosen?


amino acid substitution matrix

interactive dotplots



each transformer layer consists of multiple self-attention heads, and the Feed-forward NN combines them.

which embedding do we use? use only the embedding of the last layer

diff between (LSTM-based) seqvec and lstm


logistic regression
at the end


an average protein idea is in each aa.


1K seqs = 1GB
uniprot: maybe twice at much




........... no Machine Learning pipeline




train-test split now.
cross-validation later, but is supposedly easy with scikit
